{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nmi3CEhER8y5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import datetime\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFxO-ciP_c4r",
        "outputId": "019da3f6-5f22-4d7d-bdfd-1378df93e852"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-06-18 14:44:43.432673: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1750250683.448462   14587 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1750250683.453529   14587 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1750250683.466383   14587 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750250683.466400   14587 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750250683.466401   14587 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1750250683.466403   14587 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-06-18 14:44:43.470704: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1750250686.142106   14587 gpu_device.cc:2019] Created device /device:GPU:0 with 9711 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:05:00.0, compute capability: 8.6\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "if device_name != \"/device:GPU:0\":\n",
        "    raise SystemError(\"GPU device not found\")\n",
        "\n",
        "print(\"Found GPU at: {}\".format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joYzOG-2AQUu"
      },
      "source": [
        "# Training Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69fXWquCARHJ"
      },
      "source": [
        "## Environment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data source and training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Number of samples processed before the model is updated.\\n\\nA larger batch size typically increases memory usage (since more data is loaded into memory at once) but can speed up training per epoch due to more efficient computation on modern hardware. However, very large batch sizes may require more GPU/CPU memory than available and can sometimes negatively impact model generalization. Smaller batch sizes use less memory and may generalize better, but training can be slower per epoch due to less efficient hardware utilization.\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "SOURCE = \"mine_logs_v1\"\n",
        "'''Name of the dataset (used to build file paths for data input/output)'''\n",
        "\n",
        "ARCHITECTURE = \"flor\"       \n",
        "'''Model architecture to use (e.g., 'flor' for a specific network design)'''\n",
        "\n",
        "EPOCHS = 1000       \n",
        "'''Number of times the entire training dataset will be passed through the model'''\n",
        "\n",
        "BATCH_SIZE = 16     \n",
        "'''Number of samples processed before the model is updated.\n",
        "\n",
        "A larger batch size typically increases memory usage (since more data is loaded into memory at once) but can speed up training per epoch due to more efficient computation on modern hardware. However, very large batch sizes may require more GPU/CPU memory than available and can sometimes negatively impact model generalization. Smaller batch sizes use less memory and may generalize better, but training can be slower per epoch due to less efficient hardware utilization.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data paths and charset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Eex7kEX1ATDY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "source: data/mine_logs_v1.hdf5\n",
            "output output/mine_logs_v1/flor/\n",
            "target output/mine_logs_v1/flor/checkpoint_weights.weights.h5\n",
            "charset: 0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ ČčĆćĐđŠšŽž\n"
          ]
        }
      ],
      "source": [
        "SOURCE_PATH = \"data/\" + f\"{SOURCE}.hdf5\"\n",
        "'''Path to the input HDF5 dataset file'''\n",
        "\n",
        "OUTPUT_PATH = \"output/\" + f\"{SOURCE}/\" + f\"{ARCHITECTURE}/\"\n",
        "'''Directory for model outputs (checkpoints, logs, etc.)'''\n",
        "\n",
        "CHECKPOINT_PATH = OUTPUT_PATH + \"checkpoint_weights.weights.h5\"\n",
        "'''File path for saving/loading model checkpoint weights'''\n",
        "\n",
        "INPUT_SIZE = (1024, 128, 1)\n",
        "'''Model input image size (height, width, channels)'''\n",
        "\n",
        "MAX_TEXT_LENGTH = 128\n",
        "'''Maximum number of characters per text line'''\n",
        "\n",
        "charset_base = string.printable[:95]\n",
        "'''Set of valid characters for text recognition (base charset plus special characters)'''\n",
        "charset_base = charset_base + \"ČčĆćĐđŠšŽž\"\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "# Create a directory for the dataset if it doesn't exist\n",
        "\n",
        "print(\"source:\", SOURCE_PATH)\n",
        "print(\"output:\", OUTPUT_PATH)\n",
        "print(\"checkpoint:\", CHECKPOINT_PATH)\n",
        "print(\"charset:\", charset_base)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FF5CtX79Dopj"
      },
      "source": [
        "### 3.2 DataGenerator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7e0CBP8GDpMq",
        "outputId": "0ee577cc-63c8-4126-b9b5-d154f0e3b3ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['/home/haris/miniconda3/envs/TF-Py/lib/python312.zip', '/home/haris/miniconda3/envs/TF-Py/lib/python3.12', '/home/haris/miniconda3/envs/TF-Py/lib/python3.12/lib-dynload', '', '/home/haris/miniconda3/envs/TF-Py/lib/python3.12/site-packages', '/mnt/c/AB_data_haris/vi-htr/src/']\n",
            "Train images: 765\n",
            "Validation images: 219\n",
            "Test images: 110\n"
          ]
        }
      ],
      "source": [
        "SRC_DIR_PATH = '/mnt/c/AB_data_haris/vi-htr/src/'\n",
        "# Add the source directory to the Python path to import local modules\n",
        "# THIS IS REQUIRED!\n",
        "\n",
        "import sys\n",
        "if SRC_DIR_PATH not in sys.path:\n",
        "    sys.path.append(SRC_DIR_PATH)\n",
        "\n",
        "# Verify the path was added (optional)\n",
        "print(sys.path)\n",
        "\n",
        "from data.generator import DataGenerator\n",
        "\n",
        "dtgen = DataGenerator(source=SOURCE_PATH,\n",
        "                      batch_size=BATCH_SIZE,\n",
        "                      charset=charset_base,\n",
        "                      max_text_length=MAX_TEXT_LENGTH)\n",
        "\n",
        "print(f\"Train images: {dtgen.size['train']}\")\n",
        "print(f\"Validation images: {dtgen.size['valid']}\")\n",
        "print(f\"Test images: {dtgen.size['test']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwANUm3bKfb_"
      },
      "source": [
        "### 3.3 HTRModel Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RyHMbxc_jiGN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.19.0\n"
          ]
        }
      ],
      "source": [
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "I0000 00:00:1750250687.552390   14587 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9711 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:05:00.0, compute capability: 8.6\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# create and compile HTRModel\u001b[39;00m\n\u001b[32m      4\u001b[39m model = HTRModel(architecture=ARCHITECTURE,\n\u001b[32m      5\u001b[39m                  input_size=INPUT_SIZE,\n\u001b[32m      6\u001b[39m                  vocab_size=dtgen.tokenizer.vocab_size,\n\u001b[32m   (...)\u001b[39m\u001b[32m      9\u001b[39m                  reduce_tolerance=\u001b[32m15\u001b[39m,\n\u001b[32m     10\u001b[39m                  reduce_factor=\u001b[32m0.1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m model.summary(OUTPUT_PATH, \u001b[33m\"\u001b[39m\u001b[33msummary.txt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# get default callbacks and load checkpoint weights file (HDF5) if exists\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/AB_data_haris/vi-htr/src/network/model.py:148\u001b[39m, in \u001b[36mHTRModel.compile\u001b[39m\u001b[34m(self, learning_rate, initial_step)\u001b[39m\n\u001b[32m    141\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m \u001b[33;03mConfigures the HTR Model for training/predict.\u001b[39;00m\n\u001b[32m    143\u001b[39m \n\u001b[32m    144\u001b[39m \u001b[33;03m:param optimizer: optimizer for training\u001b[39;00m\n\u001b[32m    145\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;66;03m# define inputs, outputs and optimizer of the chosen architecture\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m inputs, outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43marchitecture\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    151\u001b[39m     learning_rate = CustomSchedule(d_model=\u001b[38;5;28mself\u001b[39m.vocab_size + \u001b[32m1\u001b[39m, initial_step=initial_step)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/mnt/c/AB_data_haris/vi-htr/src/network/model.py:367\u001b[39m, in \u001b[36mflor\u001b[39m\u001b[34m(input_size, d_model)\u001b[39m\n\u001b[32m    364\u001b[39m shape = cnn.shape\n\u001b[32m    365\u001b[39m bgru = Reshape((shape[\u001b[32m1\u001b[39m], shape[\u001b[32m2\u001b[39m] * shape[\u001b[32m3\u001b[39m]))(cnn)\n\u001b[32m--> \u001b[39m\u001b[32m367\u001b[39m bgru = \u001b[43mBidirectional\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGRU\u001b[49m\u001b[43m(\u001b[49m\u001b[43munits\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_sequences\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbgru\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    368\u001b[39m bgru = Dense(units=\u001b[32m256\u001b[39m)(bgru)\n\u001b[32m    370\u001b[39m bgru = Bidirectional(GRU(units=\u001b[32m128\u001b[39m, return_sequences=\u001b[38;5;28;01mTrue\u001b[39;00m, dropout=\u001b[32m0.5\u001b[39m))(bgru)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF-Py/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF-Py/lib/python3.12/site-packages/keras/src/layers/layer.py:832\u001b[39m, in \u001b[36mLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    829\u001b[39m \u001b[38;5;66;03m################\u001b[39;00m\n\u001b[32m    830\u001b[39m \u001b[38;5;66;03m# 4. Call build\u001b[39;00m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._open_name_scope():\n\u001b[32m--> \u001b[39m\u001b[32m832\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_spec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m \u001b[38;5;66;03m##########################\u001b[39;00m\n\u001b[32m    835\u001b[39m \u001b[38;5;66;03m# 5. Infer training value\u001b[39;00m\n\u001b[32m    836\u001b[39m \u001b[38;5;66;03m# Training phase for `Layer.call` is set via (in order of priority):\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    842\u001b[39m \u001b[38;5;66;03m# Maintains info about the `Layer.call` stack\u001b[39;00m\n\u001b[32m    843\u001b[39m \u001b[38;5;66;03m# across nested calls.\u001b[39;00m\n\u001b[32m    844\u001b[39m call_context = \u001b[38;5;28mself\u001b[39m._get_call_context()\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF-Py/lib/python3.12/site-packages/keras/src/layers/layer.py:1394\u001b[39m, in \u001b[36mLayer._maybe_build\u001b[39m\u001b[34m(self, call_spec)\u001b[39m\n\u001b[32m   1387\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m utils.is_default(\u001b[38;5;28mself\u001b[39m.build):\n\u001b[32m   1388\u001b[39m     shapes_dict = update_shapes_dict_for_target_fn(\n\u001b[32m   1389\u001b[39m         \u001b[38;5;28mself\u001b[39m.build,\n\u001b[32m   1390\u001b[39m         shapes_dict=shapes_dict,\n\u001b[32m   1391\u001b[39m         call_spec=call_spec,\n\u001b[32m   1392\u001b[39m         class_name=\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m,\n\u001b[32m   1393\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1394\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mshapes_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1395\u001b[39m     \u001b[38;5;66;03m# Check input spec again (after build, since self.input_spec\u001b[39;00m\n\u001b[32m   1396\u001b[39m     \u001b[38;5;66;03m# may have been updated\u001b[39;00m\n\u001b[32m   1397\u001b[39m     \u001b[38;5;28mself\u001b[39m._assert_input_compatibility(call_spec.first_arg)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF-Py/lib/python3.12/site-packages/keras/src/layers/layer.py:230\u001b[39m, in \u001b[36mLayer.__new__.<locals>.build_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m obj._open_name_scope():\n\u001b[32m    229\u001b[39m     obj._path = current_path()\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[32m    232\u001b[39m signature = inspect.signature(original_build_method)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF-Py/lib/python3.12/site-packages/keras/src/layers/rnn/bidirectional.py:275\u001b[39m, in \u001b[36mBidirectional.build\u001b[39m\u001b[34m(self, sequences_shape, initial_state_shape)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbuild\u001b[39m(\u001b[38;5;28mself\u001b[39m, sequences_shape, initial_state_shape=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.forward_layer.built:\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.backward_layer.built:\n\u001b[32m    277\u001b[39m         \u001b[38;5;28mself\u001b[39m.backward_layer.build(sequences_shape)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF-Py/lib/python3.12/site-packages/keras/src/layers/layer.py:230\u001b[39m, in \u001b[36mLayer.__new__.<locals>.build_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    228\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m obj._open_name_scope():\n\u001b[32m    229\u001b[39m     obj._path = current_path()\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[32m    232\u001b[39m signature = inspect.signature(original_build_method)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF-Py/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:275\u001b[39m, in \u001b[36mRNN.build\u001b[39m\u001b[34m(self, sequences_shape, initial_state_shape)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.cell, Layer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cell.built:\n\u001b[32m    274\u001b[39m     \u001b[38;5;28mself\u001b[39m.cell.build(step_input_shape)\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuilt\u001b[49m = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stateful:\n\u001b[32m    277\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda3/envs/TF-Py/lib/python3.12/site-packages/keras/src/layers/layer.py:1464\u001b[39m, in \u001b[36mLayer.__setattr__\u001b[39m\u001b[34m(self, name, value)\u001b[39m\n\u001b[32m   1461\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__str__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1462\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.\u001b[34m__repr__\u001b[39m()\n\u001b[32m-> \u001b[39m\u001b[32m1464\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, value):\n\u001b[32m   1465\u001b[39m     \u001b[38;5;66;03m# Track Variables, Layers, Metrics, SeedGenerators.\u001b[39;00m\n\u001b[32m   1466\u001b[39m     name, value = \u001b[38;5;28mself\u001b[39m._setattr_hook(name, value)\n\u001b[32m   1467\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name != \u001b[33m\"\u001b[39m\u001b[33m_tracker\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "from network.model import HTRModel\n",
        "\n",
        "# create and compile HTRModel\n",
        "model = HTRModel(architecture=ARCHITECTURE,\n",
        "                 input_size=INPUT_SIZE,\n",
        "                 vocab_size=dtgen.tokenizer.vocab_size,\n",
        "                 beam_width=10,\n",
        "                 stop_tolerance=20,\n",
        "                 reduce_tolerance=15,\n",
        "                 reduce_factor=0.1)\n",
        "\n",
        "model.compile(learning_rate=0.001)\n",
        "model.summary(OUTPUT_PATH, \"summary.txt\")\n",
        "\n",
        "# get default callbacks and load checkpoint weights file (HDF5) if exists\n",
        "model.load_checkpoint(target=CHECKPOINT_PATH)\n",
        "\n",
        "callbacks = model.get_callbacks(logdir=OUTPUT_PATH, checkpoint=CHECKPOINT_PATH, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zD619eNndNFz"
      },
      "source": [
        "## 4 Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "s-ZIWZJNdOB1",
        "outputId": "8d8cb20f-bc1b-483b-cebd-b8df79707868"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1750250166.107443   13845 meta_optimizer.cc:967] layout failed: INVALID_ARGUMENT: Size of values 0 does not match size of permutation 4 @ fanin shape inStatefulPartitionedCall/functional_1/dropout_1/stateless_dropout/SelectV2-2-TransposeNHWCToNCHW-LayoutOptimizer\n",
            "I0000 00:00:1750250166.956665   14283 cuda_dnn.cc:529] Loaded cuDNN version 90300\n"
          ]
        }
      ],
      "source": [
        "# to calculate total and average time per epoch\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "h = model.fit(x=dtgen.next_train_batch(),\n",
        "              epochs=EPOCHS,\n",
        "              steps_per_epoch=dtgen.steps['train'],\n",
        "              validation_data=dtgen.next_valid_batch(),\n",
        "              validation_steps=dtgen.steps['valid'],\n",
        "              callbacks=callbacks,\n",
        "              shuffle=True,\n",
        "              verbose=1)\n",
        "\n",
        "total_time = datetime.datetime.now() - start_time\n",
        "\n",
        "loss = h.history['loss']\n",
        "val_loss = h.history['val_loss']\n",
        "\n",
        "min_val_loss = min(val_loss)\n",
        "min_val_loss_i = val_loss.index(min_val_loss)\n",
        "\n",
        "time_epoch = (total_time / len(loss))\n",
        "total_item = (dtgen.size['train'] + dtgen.size['valid'])\n",
        "\n",
        "t_corpus = \"\\n\".join([\n",
        "    f\"Total train images:      {dtgen.size['train']}\",\n",
        "    f\"Total validation images: {dtgen.size['valid']}\",\n",
        "    f\"Batch:                   {dtgen.batch_size}\\n\",\n",
        "    f\"Total time:              {total_time}\",\n",
        "    f\"Time per epoch:          {time_epoch}\",\n",
        "    f\"Time per item:           {time_epoch / total_item}\\n\",\n",
        "    f\"Total epochs:            {len(loss)}\",\n",
        "    f\"Best epoch               {min_val_loss_i + 1}\\n\",\n",
        "    f\"Training loss:           {loss[min_val_loss_i]:.8f}\",\n",
        "    f\"Validation loss:         {min_val_loss:.8f}\"\n",
        "])\n",
        "\n",
        "with open(os.path.join(OUTPUT_PATH, \"train.txt\"), \"w\") as lg:\n",
        "    lg.write(t_corpus)\n",
        "    print(t_corpus)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kXoi5cPoIRlS",
        "w-0Leg5w_-Zp"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "TF-Py",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
